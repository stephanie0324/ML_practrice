{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pack_sequence, pad_packed_sequence\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "#from torch.optim.lr_scheduler import LambdaLR\n",
    "#import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "#import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "# # Accelerate parts\n",
    "# from accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\n",
    "# from accelerate.utils import set_seed # reproducability across devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = X\n",
    "        if y is not None:\n",
    "            # self.label = torch.LongTensor(y)\n",
    "            self.label = y\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def totalSeqLen(self):\n",
    "        x_seq_len_list = [s.shape[0] for s in self.data]\n",
    "        return sum(x_seq_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feat(path):\n",
    "    import torch\n",
    "    feat = torch.load(path)\n",
    "    return feat\n",
    "\n",
    "def preprocess_data(split, feat_dir, phone_path, train_ratio=0.8, random_seed=1213):\n",
    "\n",
    "    if split == 'train' or split == 'val':\n",
    "        mode = 'train'\n",
    "    elif split == 'test':\n",
    "        mode = 'test'\n",
    "    else:\n",
    "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
    "\n",
    "    label_dict = {}\n",
    "    if mode == 'train':\n",
    "        for line in open(os.path.join(phone_path, f'{mode}_labels.txt')).readlines():\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
    "        \n",
    "        # split training and validation data\n",
    "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
    "        train_len = int(len(usage_list) * train_ratio)\n",
    "        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n",
    "    elif mode == 'test':\n",
    "        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
    "\n",
    "    usage_list = [line.strip('\\n') for line in usage_list]\n",
    "\n",
    "    x_tensor_list = []\n",
    "    if mode == 'train':\n",
    "        y_tensor_list = []\n",
    "\n",
    "    idx = 0\n",
    "    for i, fname in enumerate(usage_list):\n",
    "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
    "        cur_len = len(feat)\n",
    "        if mode == 'train':\n",
    "            label = label_dict[fname]\n",
    "\n",
    "        x_tensor_list.append(feat)\n",
    "        if mode == 'train':\n",
    "            y_tensor_list.append(label)\n",
    "          \n",
    "\n",
    "    X = x_tensor_list\n",
    "    if mode == 'train':\n",
    "        y = y_tensor_list\n",
    "\n",
    "    if mode == 'train':\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def collate_fn(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=41, hidden_layers=4, hidden_dim=256, batch_size = 8):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout (dropout)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2 )\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, hidden_layers, dropout=dropout, bidirectional = True, batch_first = True)\n",
    "        self.bc =  nn.Sequential(\n",
    "            nn.Dropout (dropout),\n",
    "            nn.Linear(hidden_dim *2, output_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_lenght_list):\n",
    "        h0 = torch.randn(self.hidden_layers*2, x.shape[0], self.hidden_dim).to('cuda')\n",
    "        c0 = torch.randn(self.hidden_layers*2, x.shape[0], self.hidden_dim).to('cuda')\n",
    "        x = self.fc(x)\n",
    "        x_pad =pack_padded_sequence(x, batch_first=True, lengths=seq_lenght_list).to('cuda')\n",
    "        out, (hn, cn) = self.lstm(x_pad,(h0,c0))\n",
    "        out = pad_packed_sequence(out,batch_first=True)\n",
    "        out = self.layer_norm(out[0])\n",
    "        out = self.bc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLstm + wandb log\n",
    "concat_nframes = 1              # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n",
    "train_ratio = 0.75               # the ratio of data used for training, the rest will be used for validation\n",
    "\n",
    "# training parameters\n",
    "seed = 1213                        # random seed\n",
    "batch_size = 8# batch size\n",
    "num_epoch = 30                   # the number of training epoch\n",
    "learning_rate = 2e-3         # learning rate\n",
    "model_path = './model.ckpt'     # the path where the checkpoint will be saved\n",
    "\n",
    "# model parameters\n",
    "input_dim = 39 * concat_nframes # the input dim of the model, you should not change the value\n",
    "hidden_layers = 7               # the number of hidden layers\n",
    "hidden_dim = 256              # the hidden dim\n",
    "dropout = 0.35\n",
    "weight_decay = 0.05\n",
    "\n",
    "\n",
    "feat_dir = './data/libriphone/feat/'\n",
    "phone_path = './data/libriphone/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess data\n",
    "train_X, train_y = preprocess_data(split='train', feat_dir=feat_dir, phone_path=phone_path, train_ratio=train_ratio, random_seed=seed)\n",
    "val_X, val_y = preprocess_data(split='val', feat_dir=feat_dir, phone_path=phone_path,  train_ratio=train_ratio, random_seed=seed)\n",
    "\n",
    "# get dataset\n",
    "train_set = LibriDataset(train_X, train_y)\n",
    "val_set = LibriDataset(val_X, val_y)\n",
    "# remove raw feature to save memory\n",
    "del train_X, train_y, val_X, val_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "model = LstmClassifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim)\n",
    "\n",
    "#if(os.path.exists(model_path)):       \n",
    "#    model.load_state_dict(torch.load(model_path))\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "# 定义一个自定义函数来计算学习率\n",
    "#def lr_lambda(epoch):\n",
    "#    if epoch < 10:\n",
    "#        return epoch / 10\n",
    "#    else:\n",
    "#        return 0.5 * (1 + math.cos(math.pi * (epoch - 10) / 20))\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/322 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-717ffafd4402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pad_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_seq_len_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_seq_len_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my_seq_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_seq_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(40):\n",
    "    print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        batch.sort(key= lambda batch: len(batch[0]), reverse=True) \n",
    "        x_seq_list = [dataItem[0] for dataItem in batch] \n",
    "        y_seq_list = [dataItem[1] for dataItem in batch]\n",
    "\n",
    "        x_seq_len_list = [s.shape[0] for s in x_seq_list]\n",
    "        x_pad_seq = pad_sequence(x_seq_list, batch_first=True) \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(x_pad_seq, x_seq_len_list, device = device) \n",
    "        result = torch.cat([outputs[i][:x_seq_len_list[i]] for i in range(outputs.size(0))], dim=0)\n",
    "        y_seq_tensor = torch.cat(y_seq_list, dim=0)\n",
    "        # print(f\"reslut shape {result.shape} , y_seq_tensor shape {y_seq_tensor.shape}\")\n",
    "        loss = criterion(result, y_seq_tensor)\n",
    "            \n",
    "        #======================================================================\n",
    "        #attention here! \n",
    "        loss.backward() \n",
    "        #======================================================================\n",
    "\n",
    "\n",
    "        _, train_pred = torch.max(result, 1) # get the index of the class with the highest probability\n",
    "        train_acc_batch = (train_pred.detach() == y_seq_tensor.detach()).sum().item()\n",
    "        train_acc += train_acc_batch\n",
    "        train_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f'Train Acc: {train_acc_batch/y_seq_tensor.shape[0]} Loss: {loss.item()}')\n",
    "    # validation\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(val_loader)):\n",
    "            \n",
    "            batch.sort(key= lambda batch: len(batch[0]), reverse=True) \n",
    "            x_seq_list = [dataItem[0] for dataItem in batch] \n",
    "            y_seq_list = [dataItem[1] for dataItem in batch]\n",
    "\n",
    "            x_seq_len_list = [s.shape[0] for s in x_seq_list]\n",
    "            x_pad_seq = pad_sequence(x_seq_list, batch_first=True)\n",
    "\n",
    "            outputs = model(x_pad_seq, x_seq_len_list, device = accelerator.device)\n",
    "\n",
    "            result = torch.cat([outputs[i][:x_seq_len_list[i]] for i in range(outputs.size(0))], dim=0)\n",
    "            y_seq_tensor = torch.cat(y_seq_list, dim=0)\n",
    "\n",
    "            loss = criterion(result, y_seq_tensor) \n",
    "\n",
    "            _, val_pred = torch.max(result, 1) \n",
    "\n",
    "            val_acc += (val_pred.cpu() == y_seq_tensor.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "            val_loss += loss.item()\n",
    "        print(f'[{epoch+1:03d}/{num_epoch:03d}] Train Acc: {train_acc/train_set.totalSeqLen():3.5f} Loss: {train_loss/len(train_loader):3.5f} | Val Acc: {val_acc/val_set.totalSeqLen():3.5f} loss: {val_loss/len(val_loader):3.5f}')\n",
    "    \n",
    "    #======================================================================\n",
    "    #print logs and save ckpt  \n",
    "    model_path=\"model.ckpt\"\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"epoch【{epoch}】@{nowtime} --> val_acc= {100 * val_acc:.2f}%\")\n",
    "    #======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steph_env",
   "language": "python",
   "name": "steph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
